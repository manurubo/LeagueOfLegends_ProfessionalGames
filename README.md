# LeagueOfLegends_ProfessionalGames

## Descripción
Este código y ficheros corresponden a la Práctica 1 de la asignatura Tipología y ciclo de vida de los datos, perteneciente al Máster Universitario de Ciencia de Datos en la UOC. En esta práctica he aplicado técnicas de web scraping avanzadas con Selenium. El lenguaje utilizado ha sido Python 3.7. Mediante las técnicas de web scraping, he generado un dataset llamado **Partidas profesionales de League of Legends correspondientes a de WORLDS, LEC y LCS 2020**, donde se obtienen las estadísticas de las partidas profesionales de League of Legends para los campeonatos mundial, europeo y norteamericano. El dataset contiene al rededor de 600 filas y más de 500 variables por mapa. Se entiende que se han de procesar los datos para poder utilizar los datos en un proyecto en función del objetivo del proyecto. 

# Autoría
El código ha sido desarrollado por Manuel Ruiz Botella en solitario. 

# Descripción de los ficheros

Todo el código de web scraping se ha realizado en Python, utilizando Selenium como módulo principal para realizar el web scraping. Se ha utilizado este módulo porque permite una navegación interactiva por la web, simulando una navegación humana. Esta navegación interactiva era primordial porque:
* La mayoría de estadísticas de partidas de gol.gg están introducidos en la web mediante scripts de javascript, por lo que es necesario que la página cargue los scripts para poder obtener los datos. Si solo se obtiene el html mediante el módulo requests, no se ejecutan los scripts, por lo que no podríamos obtener la información. 
* Hay interacción directa con la web, tanto al rellenar un formulario de inicio de sesión, como interactuando con gráficas que representan datos. Ambos se explican más adelante.
Los ficheros de código son 4, 3 de ellos se utilizan como modulos de python que contienen funciones para separar las funcionalidades desarrolladas en diversos apartados. El otro fichero es el fichero secuencial que ejecuta el web scraping. 

1. **navegate_web.py**, este fichero contiene las funcionalidades más importantes que hacen referencia al navegar en la web gol.gg y abrir o cerrar ventanas en el buscador. En este fichero, es donde me he encontrado dos puntos de dificultad añadida:
    * Por una parte, al abrir el historial de partida que accede a leagueoflegends.com, se abre en una nueva pestaña del buscador, por tanto he tenido que ser capaz de cambiar el foco a una nueva pestaña, y cuando hemos acabado con esta ser capaz de cerrar solo la pestaña y regresar a la pestaña de gol.gg. Esto lo he solucionado utilizando el atributo del driver de selenium *window_handles* para seleccionar la pestaña objetivo y la función *switch_to_window* para cambiar el foco a esta pestaña. 
    * Por otra parte, he tenido que rellenar un formulario de inicio de sesión la primera vez que se accede a un historial de partida en leagueoflegends.com, para ello he tenido que ser capaz de identificar los lugares de input mediante su selector css e introducir los datos correctos mediante la función *send_keys* de selenium, donde se indica que se quiere introducir en el campo de input. 
2. **scrape_gol.py**, este fichero contiene las funcionalidades más importantes para extraer los datos de gol.gg, tanto de la ventana del sumario como de la página de estadísticas generales. Realmente no hay gran complejidad en esta parte, solo que al tener que extraer muchos datos del sumario, he tenido que comprender y utilizar muy bien muchas maneras de seleccionar elementos web mediante selenium, ya que cada dato se extrae de una forma concreta. 
3. **scrape_mh.py**, este fichero contiene las funcionalidades más importantes para extraer los datos del historial de partida en legueoflegends.com. En este fichero hay de neuvo una extracción de los datos de una tabla, lo cual es sencillo, pero también he tenido que afrontar la mayor dificultad de todo el código. He tenido que interactuar con una gráfica de lineas y puntos, de tal manera que he tenido que seleccionar con selenium que opción de gráfica quiero visualizar y después automatizar por selenium el poner el ratón sobre cada punto de interés. Con el objetivo de que se muestre por pantalla un texto con los datos a extraer. Esto lo he realizado gracias a  que selenium tiene el modulo *Action chains* que permite interactuar con elementos web. He utilizado la función *move_to_element* para seleccionar el punto de la gráfica de la cual quiero obtener información, y después utilizar la función *perform* para poder obtener la información, que he tenido que extraer encontrandola mediante selenium con sus funciones de *find_element_by*. 
4. **scraper.py**, este fichero es el que se encarga de ejecutar el web scraping sobre la web gol.gg para extraer los datos de las partidas profesionales de League of Legends.     * Como aspecto más dificil tiene la gran navegación que he tenido que desarrollar por selenium a lo largo de la página, ya que se puede indicar en el código a que torneos se quiere acceder y para cada torneo hay varias partes, para cada parte varios partidos y para cada partido puede haber varios mapas de los que tenemos que obtener la información. Sumado a esa navegación, hay que recordar que para cada mapa, se obtiene información de dos pestañas de gol.gg y del historial de partida de leagueofelegends.com, y que al obtener información de un mapa, se tiene que volver atrás y continuar con el siguiente. 
  * El otro aspecto dificil, ha sido conseguir evitar que los errores que pueden haber por falta de datos en la web afecten al funcionamiento del script. Esto se debe a que muchas partidas pueden tener falta de algún elemento de los datos, ya sea las gráficas interactivas, todo el link de historial de partida, o algún dato concreto en gol.gg. Por tanto, he decidio aplicar varias clausulas try/except, tanto en este fichero como en el resto. Si faltan datos que no sean vitales (del sumario de gol.gg) se obtendran los datos vacios gracias a try/except. Si faltan datos vitales o hay un problema identificando alguna parte de la web durante la extracción de datos de un mapa, el mapa se salta, gracias a try/except. Ha sido bastante díficil este paso porque he tenido que recorrer varias veces el script solucionando links fallidos o falta de datos, actualmente, sin una ejecución fallida por internet, se consigue tener casi todos los partidos, solo faltan aquellos (muy pocos) que falta su información primordial.
  


    
